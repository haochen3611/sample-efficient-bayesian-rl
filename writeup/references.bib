@inproceedings{gpsinrl,
  title = {Gaussian Processes in Reinforcement Learning},
  author = {Rasmussen, CE. and Kuss, M.},
  journal = {Advances in Neural Information Processing Systems 16},
  booktitle = {Advances in Neural Information Processing Systems 16},
  pages = {751-759},
  editors = {Thrun, S., L. K. Saul, B. Sch{\"o}lkopf},
  publisher = {MIT Press},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  address = {Cambridge, MA, USA},
  month = jun,
  year = {2004},
  month_numeric = {6}
}

@book{rl_book,
 author = {Szepesvari, Csaba},
 title = {Algorithms for Reinforcement Learning},
 year = {2010},
 isbn = {1608454924, 9781608454921},
 publisher = {Morgan and Claypool Publishers},
} 

@book{gps_textbook,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, CE. and Williams, CKI.},
  pages = {248},
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  organization = {Max-Planck-Gesellschaft},
  school = {Biologische Kybernetik},
  address = {Cambridge, MA, USA},
  month = jan,
  year = {2006},
  month_numeric = {1}
}

@inproceedings{bayesbellman,
 author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
 title = {Bayes Meets Bellman: The Gaussian Process Approach to Temporal Difference Learning},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {154--161},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041858},
 acmid = {3041858},
 publisher = {AAAI Press},
} 

@inproceedings{rlwithgps,
 author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
 title = {Reinforcement Learning with Gaussian Processes},
 booktitle = {Proceedings of the 22Nd International Conference on Machine Learning},
 series = {ICML '05},
 year = {2005},
 isbn = {1-59593-180-5},
 location = {Bonn, Germany},
 pages = {201--208},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1102351.1102377},
 doi = {10.1145/1102351.1102377},
 acmid = {1102377},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@inproceedings{pilco,
  title = {PILCO: A Model-Based and Data-Efficient Approach to Policy Search},
  author = {Deisenroth, MP. and Rasmussen, CE.},
  booktitle = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  pages = {465-472},
  editors = {L Getoor and T Scheffer},
  publisher = {Omnipress},
  year = {2011}
}

@inproceedings{deisenroth_thesis,
  title={Efficient reinforcement learning using Gaussian processes},
  author={Marc Peter Deisenroth},
  year={2010}
}

@book{bishop,
 author = {Bishop, Christopher M.},
 title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
 year = {2006},
 isbn = {0387310738},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 


@INPROCEEDINGS{bqlearning,
    author = {Richard Dearden and Nir Friedman and Stuart Russell},
    title = {Bayesian Q-learning},
    booktitle = {In AAAI/IAAI},
    year = {1998},
    pages = {761--768},
    publisher = {AAAI Press}
}

@incollection{psrl,
title = {(More) Efficient Reinforcement Learning via Posterior Sampling},
author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3003--3011},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling.pdf}
}


@article{ube,
  author    = {Brendan O'Donoghue and
               Ian Osband and
               R{\'{e}}mi Munos and
               Volodymyr Mnih},
  title     = {The Uncertainty Bellman Equation and Exploration},
  journal   = {CoRR},
  volume    = {abs/1709.05380},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.05380},
  archivePrefix = {arXiv},
  eprint    = {1709.05380},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-05380},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{suttonbarto,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@book{bellmanoriginal,
  author = {Bellman, Richard},
  biburl = {https://www.bibsonomy.org/bibtex/29cdd821222218ded252c8ba5cd712666/pcbouman},
  interhash = {acf948462171ca060064a7ded257a792},
  intrahash = {9cdd821222218ded252c8ba5cd712666},
  isbn = {9780486428093},
  keywords = {book dynamic programming},
  publisher = {Dover Publications},
  timestamp = {2011-08-18T09:10:27.000+0200},
  title = {{Dynamic Programming}},
  year = 1957
}


@TECHREPORT{murphy,
    author = {Kevin P. Murphy},
    title = {Conjugate bayesian analysis of the gaussian distribution},
    institution = {},
    year = {2007}
}


@Article{qlearning,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698"
}


@TECHREPORT{iothesis,
    author = {Ian Osband},
    title = {Deep exploration via randomised value functions (PhD thesis)},
    institution = {University of Stanford},
    year = {2016}
}


@misc{SU,
title={Successor Uncertainties: exploration and uncertainty in temporal difference learning},
author={David Janz and Jiri Hron and José Miguel Hernández-Lobato and Katja Hofmann and Sebastian Tschiatschek},
year={2019},
url={https://openreview.net/forum?id=BklACjAqFm},
}

@inproceedings{distperrl,
  author    = {Marc G. Bellemare and
               Will Dabney and
               R{\'{e}}mi Munos},
  title     = {A Distributional Perspective on Reinforcement Learning},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  pages     = {449--458},
  year      = {2017},
  crossref  = {DBLP:conf/icml/2017},
  url       = {http://proceedings.mlr.press/v70/bellemare17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/icml/BellemareDM17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{iqn,
  title = 	 {Implicit Quantile Networks for Distributional Reinforcement Learning},
  author = 	 {Dabney, Will and Ostrovski, Georg and Silver, David and Munos, Remi},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1096--1105},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/dabney18a/dabney18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/dabney18a.html}
}



@article{sarsa_conv,
 author = {Singh, Satinder and Jaakkola, Tommi and Littman, Michael L. and Szepesv\'{a}ri, Csaba},
 title = {Convergence Results for Single-Step On-PolicyReinforcement-Learning Algorithms},
 journal = {Mach. Learn.},
 issue_date = {March 2000},
 volume = {38},
 number = {3},
 month = mar,
 year = {2000},
 issn = {0885-6125},
 pages = {287--308},
 numpages = {22},
 url = {https://doi.org/10.1023/A:1007678930559},
 doi = {10.1023/A:1007678930559},
 acmid = {343202},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {Markov decision processes, convergence, on-policy, reinforcement-learning},
} 

@article{thompson,
    author = {Thompson, William R.},
    title = "{On the likelihood that one unknown probability exceeds another in view of the evidence of two samples}",
    journal = {Biometrika},
    volume = {25},
    number = {3-4},
    pages = {285-294},
    year = {1933},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/25.3-4.285},
    url = {https://doi.org/10.1093/biomet/25.3-4.285},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/25/3-4/285/513725/25-3-4-285.pdf},
}


@inproceedings{thompson_empirical,
  added-at = {2014-12-10T00:00:00.000+0100},
  author = {Chapelle, Olivier and Li, Lihong},
  biburl = {https://www.bibsonomy.org/bibtex/2efb9273ac543c6e752c30f25c1b3ee43/dblp},
  booktitle = {NIPS},
  crossref = {conf/nips/2011},
  editor = {Shawe-Taylor, John and Zemel, Richard S. and Bartlett, Peter L. and Pereira, Fernando C. N. and Weinberger, Kilian Q.},
  ee = {http://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling},
  interhash = {1ddd4b1bb1bc40bbf8cab790030fbca8},
  intrahash = {efb9273ac543c6e752c30f25c1b3ee43},
  keywords = {dblp},
  pages = {2249-2257},
  timestamp = {2015-06-19T08:11:38.000+0200},
  title = {An Empirical Evaluation of Thompson Sampling.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2011.html#ChapelleL11},
  year = 2011
}

@article{thompson_theoretical,
 author = {Agrawal, Shipra and Goyal, Navin},
 title = {Near-Optimal Regret Bounds for Thompson Sampling},
 journal = {J. ACM},
 issue_date = {October 2017},
 volume = {64},
 number = {5},
 month = sep,
 year = {2017},
 issn = {0004-5411},
 pages = {30:1--30:24},
 articleno = {30},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/3088510},
 doi = {10.1145/3088510},
 acmid = {3088510},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Multi-armed bandits},
} 

@book{silver,
 author = {Silver, David},
 title = {Reinforcement Learning},
 year = {2015},
 publisher = {University College London}
} 

@book{bertsekas,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P and Bertsekas, Dimitri P},
  volume={1},
  number={2},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

@TECHREPORT{pomdp,
    author = {Kevin P. Murphy},
    title = {A Survey of POMDP Solution Techniques},
    institution = {},
    year = {2000}
}


@incollection{urcl2,
title = {Near-optimal Regret Bounds for Reinforcement Learning},
author = {Peter Auer and Thomas Jaksch and Ronald Ortner},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {89--96},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3401-near-optimal-regret-bounds-for-reinforcement-learning.pdf}
}

@InProceedings{agrawal_thompson,
  title = 	 {Analysis of Thompson Sampling for the Multi-armed Bandit Problem},
  author = 	 {Shipra Agrawal and Navin Goyal},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {39.1--39.26},
  year = 	 {2012},
  editor = 	 {Shie Mannor and Nathan Srebro and Robert C. Williamson},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf},
  url = 	 {http://proceedings.mlr.press/v23/agrawal12.html}
}

@article{deepsea,
  author    = {Brendan O'Donoghue},
  title     = {Variational Bayesian Reinforcement Learning with Regret Bounds},
  journal   = {CoRR},
  volume    = {abs/1807.09647},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.09647},
  archivePrefix = {arXiv},
  eprint    = {1807.09647},
  timestamp = {Mon, 13 Aug 2018 16:49:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-09647},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{rand_val_func,
  author    = {Ian Osband and
               Daniel Russo and
               Zheng Wen and
               Benjamin Van Roy},
  title     = {Deep Exploration via Randomized Value Functions},
  journal   = {CoRR},
  volume    = {abs/1703.07608},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07608},
  archivePrefix = {arXiv},
  eprint    = {1703.07608},
  timestamp = {Mon, 13 Aug 2018 16:48:11 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Osband0WR17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@incollection{atkenson,
title = {The Parti-Game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-Spaces},
author = {Andrew W. Moore},
booktitle = {Advances in Neural Information Processing Systems 6},
editor = {J. D. Cowan and G. Tesauro and J. Alspector},
pages = {711--718},
year = {1994},
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/742-the-parti-game-algorithm-for-variable-resolution-reinforcement-learning-in-multidimensional-state-spaces.pdf}
}


@article{mnihatari,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}

@article{alphago,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}


@article{endtoendpolicies,
 author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
 title = {End-to-end Training of Deep Visuomotor Policies},
 journal = {J. Mach. Learn. Res.},
 issue_date = {January 2016},
 volume = {17},
 number = {1},
 month = jan,
 year = {2016},
 issn = {1532-4435},
 pages = {1334--1373},
 numpages = {40},
 url = {http://dl.acm.org/citation.cfm?id=2946645.2946684},
 acmid = {2946684},
 publisher = {JMLR.org},
 keywords = {neural networks, optimal control, reinforcement learning, vision},
} 


@incollection{rldialogue,
title = {Reinforcement Learning for Spoken Dialogue Systems},
author = {Satinder P. Singh and Michael J. Kearns and Diane J. Litman and Marilyn A. Walker},
booktitle = {Advances in Neural Information Processing Systems 12},
editor = {S. A. Solla and T. K. Leen and K. M\"{u}ller},
pages = {956--962},
year = {2000},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1775-reinforcement-learning-for-spoken-dialogue-systems.pdf}
}

@conference{shanegu,
  title = {Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  author = {Gu, Shixiang and Holly, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle = {Proceedings 2017 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  month = may,
  year = {2017},
  month_numeric = {5}
}

@article{alphazero,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
               Learning Algorithm},
  journal   = {CoRR},
  volume    = {abs/1712.01815},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01815},
  archivePrefix = {arXiv},
  eprint    = {1712.01815},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-01815},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{BBBoracle,
 author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
 title = {Weight Uncertainty in Neural Networks},
 booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
 series = {ICML'15},
 year = {2015},
 location = {Lille, France},
 pages = {1613--1622},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=3045118.3045290},
 acmid = {3045290},
 publisher = {JMLR.org},
} 


@TECHREPORT{thrun,
    author = {Sebastian B. Thrun},
    title = {Efficient Exploration In Reinforcement Learning},
    institution = {},
    year = {1992}
}

@book{weiss,
  title={A Course in Probability},
  author={Weiss, N.A. and Holmes, P.T. and Hardy, M.},
  isbn={9780201774719},
  lccn={2004051068},
  url={https://books.google.co.uk/books?id=Be9fJwAACAAJ},
  year={2006},
  publisher={Pearson Addison Wesley}
}


@TECHREPORT{quintechrep,
    author = {Joaquin Quiñonero-Candela and Agathe Girard and Carl Edward Rasmussen},
    title = {Prediction at an Uncertain Input for Gaussian Processes and Relevance Vector Machines Application to Multiple-Step Ahead Time-Series Forecasting},
    institution = {},
    year = {2002}
}


@inproceedings{heteroscedastic,
 author = {L\'{a}zaro-Gredilla, Miguel and Titsias, Michalis K.},
 title = {Variational Heteroscedastic Gaussian Process Regression},
 booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
 series = {ICML'11},
 year = {2011},
 isbn = {978-1-4503-0619-5},
 location = {Bellevue, Washington, USA},
 pages = {841--848},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3104482.3104588},
 acmid = {3104588},
 publisher = {Omnipress},
 address = {USA},
} 


@article{ofumunos,
  author    = {R{\'{e}}mi Munos},
  title     = {From Bandits to Monte-Carlo Tree Search: The Optimistic Principle
               Applied to Optimization and Planning},
  journal   = {Foundations and Trends in Machine Learning},
  volume    = {7},
  number    = {1},
  pages     = {1--129},
  year      = {2014},
  url       = {https://doi.org/10.1561/2200000038},
  doi       = {10.1561/2200000038},
  timestamp = {Sat, 20 May 2017 00:22:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/ftml/Munos14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@InProceedings{samplingbetterthanoptimism,
  title = 	 {Why is Posterior Sampling Better than Optimism for Reinforcement Learning?},
  author = 	 {Ian Osband and Van Roy, Benjamin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2701--2710},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/osband17a/osband17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/osband17a.html}
}


@inproceedings{deeppilco,
  title={Improving PILCO with Bayesian Neural Network Dynamics Models},
  author={Rowan McAllister and Carl E. Rasmussen},
  year={2016}
}
