\documentclass{article}

\input{preamble}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     \usepackage{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{apalike}



\title{Bayesian methods for efficient\\Reinforcement Learning in tabular problems}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Duffy Duck\thanks{Additional information about author} \\
  Disney Studios\\
  \texttt{duffy@duck.com} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\begin{enumerate}
\item \st{Write Bayesian algorithms intro.}
\item \st{Environment and experiment description.}
\item Results and discussion.
\item Conclusions.
\item \st{Fix algorithm loops.}
\item \st{Check action notation in moment matching.}
\item \st{Add agent parameter details.}
\item \st{Change w to z equation in appendix.}
\item \st{UBE: plots of types of uncertainty.}
\item Add regret summary plots.
\end{enumerate}

Points to drive home
\begin{enumerate}
\item \st{Posterior uncertainty guides exploration. Agent does not need to be certain, just sure enough about the optimal action.}
\item \st{UBE grossly over-estimates uncertainty and over-weighs dynamics uncertainty - important to calibrate $\zeta$.}
\item \st{BQL may suffer from bad updates and lack of a forgetting mechanism.}
\item \st{MM produces well calibrated uncertainties in this setting, no need to tune $\zeta$.}
\item \st{Correlations are very important for optimal action selection.}
\item \st{No clear computational advantage of any of the methods over PSRL.}
\item More sophisticated action-selection schemes are possible.
\end{enumerate}

\clearpage
\maketitle

\begin{abstract}
The exploration-exploitation tradeoff is one of the central problems of Reinforcement Learning (RL). We explore how Bayesian modelling can be incorporated into RL to tackle this tradeoff, by quantifying relevant epistemic uncertainties and using them to efficiently guide the exploration. Empirical results of four Bayesian methods in the tabular setting - Bayesian Q-Learning (BQL), posterior sampling for RL (PSRL), the uncertainty Bellman equation (UBE) and our own moment matching (MM) approach - shows evidence that: BQL may suffer from a pathology whereby early incorrect posterior updates result in an overconfident and inaccurate posterior; the UBE greatly over-estimates uncertainties and places a much heavier emphasis on the dynamics than the rewards uncertainties; MM gives generally well-calibrated uncertainty estimates; factored posterior approximations (BQL, UBE, MM) have adverse effects on regret performance while PSRL, which involves posterior correlations, does not face the same issue.
\end{abstract}

\section{Introduction}

\subsection{Motivation}

Balancing exploration and exploitation is one of the central challenges in Reinforcement Learning (RL). On one hand, the agent should \textit{exploit} regions of its environment which are known to be rewarding, while on the other it should \textit{explore} in hope of larger rewards (\cite{suttonbarto}). Excessively exploitative or explorative behaviours are both suboptimal. In the former, the agent will fixate on small rewards and will be slow to discover the optimal policy. In the latter, it will keep exploring and making suboptimal moves, even though the observed data are already sufficient to confidently determine the optimal policy.

A guarantee for sufficient exploration is a crucial part of every RL algorithm. For example, Q-Learning (\cite{qlearning}) converges to the true $Q^*$-values, provided among other conditions, that every state-action is visited infinitely often in the limit $t \to \infty$. To guarantee sufficient exploration, $\epsilon$-greedy or Boltzmann (\cite{suttonbarto}) approaches are traditionally used. However, as demonstrated by \cite{iothesis}, such schemes can be very slow to learn, because their exploration is \textit{undirected}: instead of considering the agent's \textit{uncertainty} and they drive exploration by injecting random noise in action selection. Further, robust methods for annealing the exploration parameters ($\epsilon$ or $T$) have yet to be found in the literature and most practical applications do not use annealing at all (\cite{mnihatari}), at the expense of crude exploration schemes.

To explore efficiently, action-selection must be \textit{directed}: it must be guided by a quantification of the agent's uncertainty - Bayesian modelling is a natural framework for this quantification. By representing the agent's posterior beliefs and selecting actions accordingly, the exploration becomes guided by the degree of uncertainty. Further, such an approach offers an intuitive and principled \textit{transition mechanism} from exploration to exploitation: the posteriors shrink and the agent converges to the optimal policy as further data are observed. In this work we present a number of Bayesian algorithms in tabular Markov Decision Processes (MDPs) including our own approach. We compare the algorithms' behaviour and explain differences in performance, yielding several important insights.

\subsection{Notation convention}

We find it valuable to introduce a general notation for our discussion. The MDP $\langle \mct, \mcr, \mcs, \mca, \phi, T \rangle$ is defined by the dynamics and rewards distributions $\mct \equiv p(\ns | \s, \ac)$ and $\mcr \equiv p(r | \ns, \s, \ac)$, state and action spaces $\mcs$ and $\mca$, initial-state distribution $\phi$ and episode duration $T$ ($T = \infty$ for continuing tasks). We use $\s, \ac, r, \ns$ interchangeably with $\s_t, \ac_t, r_t, \s_{t+1}$ for states, actions, rewards and next-states, $\pi$ for the policy and $\pi^*$ for the optimal or greedy policy. In addition to $\V$ and $\Q$ to denote state and action values under $\pi$, we define the state and action \textit{return} random variables $\wb$ and $\zb$,
\begin{align}
\wb \equiv \sum_{t = 1}^T \gamma^{~t - 1} r_t \big| \pi, \s_1 = \s, \mct, \mcr ~~\text{ and }~~\zb &\equiv \sum_{t = 1}^T \gamma^{~t - 1} r_t \big| \pi, \s_1 = \s, \ac_1 = \ac, \mct, \mcr.
\end{align}
These are the cumulative discounted rewards received by following $\pi$ from $\s$, or executing $\ac$ from $\s$ and following $\pi$ thereafter, respectively. We use $\mcw^\pi$ and $\mcz^\pi$ to denote the corresponding distributions.

\section{Types of uncertainty: epistemic and aleatoric}

Distributional RL (DRL) (\cite{distperrl}) is a recent method leveraging the fact that the action-return is a random variable. The authors consider the \textit{distributional BE}:
\begin{align} \label{eq:distributional}
\zb = \rsanb + \gamma \znb
\end{align}
where $\ns \sim \mct$, $\rsanb \sim \mcr$, $\ac' \sim \pi(\s)$, and equality means the two sides are identically distributed. Where traditional algorithms such as Q-Learning aim at learning $Q^*$, DRL learns the distribution of $\zob$, denoted $\mczo$, whose expectation is $Q^*_{\mathbf{s}, \mathbf{a}}$. \cite{distperrl} postulate that DRL improves performance because it takes advantage of a richer learning signal. Whole distributions over returns are modelled instead of just their means so DRL can gracefully handle multi-modalities in the return.

DRL models the \textit{aleatoric} or \textit{irreducible} uncertainty due to the inherent stochasticity in $\mct$ and $\mcr$. Even if the agent knows $\mct$ and $\mcr$ exactly, it will not be able to perfectly predict $\zob$ if $\mct$ and $\mcr$ are stochastic. Modelling the aleatoric uncertainty may lead to more meaningful models of the return but is not useful for improving exploration. In addition to aleatoric uncertainty, there will also be uncertainty about the parameterisation of $\mczo$ due to the finite amount of data collected by the agent, known as \textit{epistemic} uncertainty. This decreases as more data are observed and expresses the agent's belief for quantities such as the \textit{expected returns}. The agent should therefore take this reducible uncertainty into account when exploring, since actions may be better or worse the current estimate.

One plausible and principled approach for balancing exploration and exploitation is quantify the epistemic uncertainty and incorporate it into action selection, for example by Thompson sampling (\cite{thompson}). This approach directs exploration according to the amount of reducible uncertainty and also provides a smooth transition into exploitation, as the posteriors become narrower. 

\subsection{Bayesian modelling and the Bellman equations}

In both the model-based and model-free settings, we are interested in representing the agent's posterior beliefs about $\mct$, $\mcr$, $\mcw$ or $\mcz$. We parameterise relevant distributions with parameters $\btheta$, and will given data $\data = \{\s, \ac, \ns, r\}$ we want to obtain $p(\btheta | \data)$. Bayes' rule allows us to do this, so long as we provide a prior $p(\btheta)$:
\begin{align}
p(\btheta | \data) = \frac{p(\data | \btheta) p(\btheta)}{p(\data)}.
\end{align}
Choosing a \textit{conjugate} prior simplifies downstream calculations: for discrete distributions such as $\mct$, we use a Categorical-Dirichlet model (\cite{bishop}) for each $\s, \ac$, while for continuous distributions such as $\mcr, \mcw, \mcz$ we use a Normal-NG model (\cite{murphy}) for each $\s, \ac, \ns$.


\section{Bayesian RL algorithms}
\subsection{Bayesian Q-Learning}
Bayesian Q-Learning (BQL) (\cite{bqlearning}) is a model-free approach for the tabular setting. The agent models the distribution over returns under the optimal policy, $\mczo$, and updates $p(\thetamczo | \data)$ as new data arrive. The authors make three modelling assumptions: (1) the return from any state-action is Gaussian; (2) the prior over the mean and precision for each of these Gaussians is Normal-Gamma (NG); (3) the NG posterior\footnote{Since $\zob$ is modelled by a Gaussian with an NG prior over its mean and precision, the posterior is also NG.} factors over different state-actions.

Although the first two are mild assumptions, the latter is more significant because it approximates the true posterior by a factored distribution. In reality, the expected returns are related though the BE, so the exact posterior is not factored. To update $p(\thetamczo | \data)$ after each transition, the authors use a mixture-of-distributions update rule and approximate this mixture by the NG closest to it in terms of KL-divergence. In our experiments, we see evidence that this update rule is problematic. Action selection can be performed by Thompson sampling. See appendix \ref{app:bql} for further details.

\subsection{Posterior sampling for reinforcement learning}

Posterior Sampling for Reinforcement Learning (PSRL) (\cite{psrl}) is an elegantly simple and yet provably efficient model-based algorithm for sampling from the exact posterior over optimal policies $p(\pi^* | \data)$. It amounts to sampling $\thetamcthat \sim \thetamctpost$ and $\thetamcrhat \sim \thetamcrpost$, and solving the BE for $\hat{Q}^* | \thetamcthat, \thetamcrhat $ and $\hat{\pi}^* | \thetamcthat, \thetamcrhat$. Policy $\hat{\pi}^*$ is then followed for a single episode, or for a pre-defined horizon in continuing tasks. \cite{psrl} prove the regret of PSRL is sub-linear. See appendix \ref{app:psrl} for further details.

\subsection{The uncertainty Bellman equation}
The Uncertainty Bellman Equation (UBE), is a model-based method proposed by \cite{ube}, for estimating the epistemic uncertainty in $\mu_{\zb}$. The authors assume that: (1) the MDP is a directed acyclic graph (DAG) and the task is episodic, with $t = 1, ..., T$ denoting the episode time-step; (2) the mean immediate rewards of the MDP are bounded within $[-R_{max}, R_{max}]$. Taking variances across the BE and defining an appropriate Bellman operator $\Ubell$, they show that the corresponding UBE:
\begin{gather*}
u_{\s, \ac, t}^\pi = \mathcal{U}^\pi_{t} u_{\s, \ac, t + 1}^\pi,\text{ where } u_{\s, \ac, T + 1}^\pi = 0
\end{gather*}
has a unique solution $u_{\s, \ac, t}^\pi$ which upper bounds the epistemic uncertainty $\Var_{\thetamct, \thetamcr} \big[ \mu_{z^{\pi}_{\s, \ac, t}} \big]$. In practice, assumption (1) must be violated to apply the UBE to non-DAG MDPs or in the continuing setting. By first solving for the greedy policy ${\pi^*}$ w.r.t. $p(\thetamct | \data)$ and $p(\thetamcr | \data)$, and then solving the UBE for $u_{\s, \ac, t}^{*}$, Thompson sampling can be performed from a diagonal Gaussian. The Thompson noise variance is $\zeta^2 u_{\s, \ac, t}^{*}$, where $\zeta$ is an appropriate scaling factor. Like BQL, this is also a factored posterior approximation. Further details are given in appendix \ref{app:ube}.

\subsection{Moment Matching across the Bellman equation}

Our moment matching (MM) approach uses the BE to estimate epistemic uncertainties, without resorting to an upper bound approximation. Instead we require equality of first and second moments across the BE. The first-order equation gives the familiar BEs. Using the laws of total variance and covariance, the second-order moments can be decomposed into purely aleatoric and purely epistemic terms. We argue that the aleatoric and epistemic terms should satisfy two separate equations.

We thus propose first solving for the greedy policy $\pi^*$ w.r.t. $p(\thetamct | \data)$ and $p(\thetamcr | \data)$, and then for the epistemic uncertainty in $\mu_{z^*_{\s, \ac}}$. The latter is used for Thompson sampling from a diagonal gaussian, resulting in a factored approximation of the posterior as in the UBE. An outline of the uncertainty decomposition and further details are given in appendix \ref{app:mm}.

\section{Environments and methods}

We compare the algorithms on three kinds of finite MDPs of variable sizes, and all experiments\footnote{Implementations of the agents and environments, as well as notebooks for plotting all figures in this work are available at \texttt{\href{https://github.com/sample-efficient-bayesian-rl/nips-2019-rl-workshop}{https://github.com/sample-efficient-bayesian-rl}}.} are in the continuing setting - exact specifications and illustrations given in \cref{app:env}. We measure performance by the cumulative regret to an oracle agent which acts under the optimal policy.

Our DeepSea MDP is a variant of those in \cite{rand_val_func, deepsea}, and is aimed at testing the algorithm's ability for sustained exploration despite initially receiving negative rewards. We also propose WideNarrow, an environment designed specifically to investigate the effect of factored posterior approximations made in BQL, UBE and MM. Finally, since the DeepSea and WideNarrow are handcrafted, we also compare the algorithms on MDPs drawn from a Dirichlet prior over $\thetamct$ and NG prior over $\thetamcr$ as in \cite{psrl} - we refer to this as PriorMDP.

\section{Results and discussion}

Visualisations of the posterior evolution and cumulative regret to an oracle on small MDPs, illustrate a number of interesting phenomena (\crefrange{ql_deepsea_visual}{mm_deepsea_visual}, \crefrange{ql_widenarrow_visual}{mm_widenarrow_visual} and \crefrange{ql_priormdp_visual}{mm_priormdp_visual}).

In many cases, we observe that as training progresses, the posteriors concentrate on the true $Q^*$ values, the behaviour policy converges on the optimal policy and the agent smoothly transitions into greedy action selection. Further, the agent does not over-explore actions if it is confident that these are suboptimal. This is notably seen in \crefrange{bql_deepsea_visual}{mm_deepsea_visual}. There, although there is significant uncertainty in the expected return of the suboptimal action, the agent is confident that the optimal action $(\s = 4, \ac = \textit{right})$ is better than the suboptimal one $(\s = 4, \ac = \textit{left})$: the agent does not spend its time determining the exact expected return of an action if it is confident that it is suboptimal. These two behaviours are central for achieving a principled and efficient approach to exploration, however we often observe exceptions where the agent does not perform in such a way.

First, the UBE uncertainty estimate $u^*_{\s, \ac}$ remains extremely loose even after a large number of time-steps (\cref{ube_deepsea_visual1}, \cref{ube_widenarrow_visual1} and \cref{ube_priormdp_visual1}). Even though $\mu_{\zob}$ be close to $Q^*$, $u^*_{\s, \ac}$ is so large that the Thompson noise completely smooths out differences between actions, which are picked almost uniformly at random. Further, $u^*_{\s, \ac}$ shrinks very slowly and the transition to greedy behaviour takes an extremely long time, causing poor regret performance. These effects are due to the contribution of an extremely large term coming from the upper-bound derivation of \cite{ube} - this is the $Q_{max}$ term in \cref{eq:bellunc} and \cref{eq:localunc}). Further, this term depends solely on the dynamics model, so $u^*_{\s, \ac}$ is dominated by the dynamics uncertainty, while the rewards uncertainty is much smaller (\cref{ube_unc_terms}). Scaling the Thompson noise by $\zeta < 1.0$, improves regret performance in some cases (e.g. \cref{ube_deepsea_visual01}). However, one is further faced by the challenge of tuning $\zeta$, which may be expensive and challenging for large problems.

By contrast, our MM approach produces more well-calibrated uncertainty estimates than the UBE (see \cref{mm_deepsea_visual}, \cref{mm_widenarrow_visual} and \cref{mm_priormdp_visual}). As a result, MM shows typically better regret performance than UBE without a need to tune $\zeta$. This could give an advantage to MM over the UBE in settings where tuning may be expensive or difficult.

Second, we observe that the BQL posterior sometimes fails to concentrate on the true $Q^*$ values (e.g. \cref{bql_deepsea_visual} and \cref{bql_priormdp_visual}), where the posterior is overconfident about incorrect predictions of $\mu_{\zob}$. This effect persists for different random seeds and is affected by the prior used. In particular, using an NG prior with a mean $\mu_0$ that is closer to the true $Q^*$ values, results in the posterior concentrating on the true $Q^*$. These effects can be explained through the update rule used in BQL (\cref{eq:mixture}). The update rule uses the next-state-action posterior $p(\zonb | \data)$ to update the current state-action posterior. If the former is inaccurate and overconfident, the updated hyperparameters are affected accordingly. BQL can hardly escape from this situation because it does not involve a \textit{forgetting mechanism} for inaccurate updates far in the past. Contrast this with $Q$-Learning, in which the Temporal Difference (TD) updates result in a moving average of observed rewards. Model-free Bayesian approaches with a rule similar to BQL may suffer from a similar pathology.

Third, there is strong evidence that factored approximations made by BQL, UBE and MM have a significant effect on regret performance. Factored approximations result in overly loose posteriors (see \cref{correlations_widenarrow_500}, \cref{correlations_widenarrow_2500}, \cref{correlations_priormdp_500} and \cref{correlations_priormdp_2500}) and as a result, the Thompson-sampled $\mu_{\zob}$ often correspond to picking a sub-optimal action\footnote{In our Thompson-sample plots, an action is optimal only if the sample lies on the same side of the black dashed line as the black cross, across all plots.}. By contrast, PSRL draws samples from the exact posterior and thereby accounts for correlations between different state-actions, which are in fact quite significant. The exact posterior often has marginals of similar scale as those of BQL or MM, however by incorporating correlations PSRL selects optimal actions much more often and thus achieves a better regret performance. Accounting for these correlations is an important factor in ensuring the transition from exploration to exploitation occurs quickly enough. PSRL typically outperforms BQL, UBE and MM as a result of these correlations at no additional computational cost.

\section{Conclusions \& Further work}

Our comparison of BQL, PSRL, UBE and MM has yielded a number of insights about these algorithms: BQL suffers from a pathology whereby incorrect posterior updates result in an overconfident posterior in the absence of a forgetting mechanism, an effect from which other model-free approaches without forgetting may suffer from; the UBE uncertainty estimate $u^*_{\s, \ac}$ is extremely loose, results in undirected exploration if $\zeta$ is not tuned and places a much larger emphasis on the dynamics than the rewards uncertainties; factored approximations to the posterior as those in BQL, UBE and MM, have adverse effects on regret performance, while PSRL does not suffer from the same phenomenon since it samples from the true posterior with correlations; MM gives generally well-calibrated uncertainty estimates, however it still suffers from the factored posterior approximation.

We conclude with points for future work:
\begin{itemize}
\item PSRL outperformed the other methods in our experiments. Inspired by this one could explore how to extend PSRL to tasks with continuous state-actions, for example by using Gaussian Process (GP) \cite{gps_textbook}.
\item MM can also be performed in continuous state-action tasks. We have conducted preliminary work for GP-based MM, using the approaches from \cite{gpsinrl, quintechrep} but further investigation is needed for this work to come to fruition.
\item Devising a principled forgetting mechanism for BQL and examining whether this remedies the observed pathology would be an interesting direction of work.
\item A comparison with other approaches such as \cite{bdqn}, \cite{su} and \cite{rlvi} would give a more complete picture of performance across a broader set of algorithms.
\item We have used Thompson sampling, however alternative action selection methods, such as those presented in \cite{bqlearning}, could be explored.
\end{itemize}

\clearpage

\bibliographystyle{apalike}
\bibliography{references}

\clearpage
\begin{appendices}
\section{Additional algorithm details}

Here we provide additional details on each algorithm, including elaborations of the assumptions made in each case and pseudocode listings. For all Dirichlet priors we use hyperparameters $\bs{\eta}_{\s, \ac} = \bm{1}$ and for all NG priors we use $(\mu_0, \lambda, \alpha, \beta)_{\s, \ac} = (0.0, 4.0, 3.0, 3.0)$.

\subsection{Bayesian Q-Learning} \label{app:bql}

\cite{bqlearning} propose the following modelling assumptions and update rule:

\textbf{Assumption 1:} The return $\zob$ is Gaussian-distributed. If the MDP is ergodic\footnote{An MDP is ergodic if, under any policy, each state-action is visited an infinite number of times and without any systematic period (\cite{silver}).} and $\gamma \approx 1$, then since the immediate rewards are independent events, one can appeal to the central limit theorem to show that $\zob$ is Gaussian-distributed. This assumption will not hold in general if the MDP is not ergodic. For example, we expect certain real world, deterministic environments to not satisfy ergodicity.

\textbf{Assumption 2:} The prior $p(\mu_{\zob}, \tau_{\zob})$ is NG, and factorises over different state-actions. This is a mild assumption, which simplifies downstream calculations.

\textbf{Assumption 3:} The posterior $p(\mu_{\zob}, \tau_{\zob}| \data)$ factors over different state-actions. This simplified distribution is a factored approximation of the true posterior. In general, we expect this assumption to fail, because we in fact know the returns from different state actions to be correlated by the BE.

\textbf{Update rule:} Suppose the agent observes a transition $\s, \ac \to \ns, r$. Assuming the agent greedily will follow the policy which it \textit{thinks} to be optimal thereafter results in the following updated posterior:
\begin{align}\label{eq:mixture}
p^{mix}_{\s, \ac}(\bqlmu, \bqltau | r, \data) = \int p(\bqlmu, \bqltau | r + \gamma \zonb, \data) p(\zonb | \data) \dint \zonb.
\end{align}
where $\ac' = \argmax_{\tilde{\ac}} z^*_{\ns, \tilde{\ac}}$. Because $p^{mix}_{\s, \ac}$ will not in general be NG-distributed, the authors propose approximating it by the NG closest to it in KL-distance. Given a distribution $q(\bqlmu, \bqltau)$, the NG $p(\bqlmu, \bqltau)$ minimising $KL(q || p)$ has parameters:
\begin{equation}
\begin{aligned} \label{eq:bqlupdate}
\bqlmuzero &= \E_q[\bqlmu \bqltau] / \E_q[\bqltau], \\
\bqllambda &= (\E_q[\bqlmu^2 \bqltau] - \E_q[\bqltau] \mu_{0_{\s, \ac}}^2)^{-1},\\
\bqlalpha &= \max \Big(1 + \epsilon, f^{-1}\rdb{\log \E_q\sqb{\bqltau} - \E_q\sqb{\log \bqltau}}\Big), \\
\bqlbeta &= \bqlalpha / \E_q\sqb{\bqltau}.
\end{aligned}
\end{equation}
where $f(x) = \log(x) - \psi(x)$ and $\psi(x) = \Gamma'(x) / \Gamma(x)$. All $\E_q$ expectations are estimated by Monte Carlo. $f^{-1}$ is analytically intractable, but can be estimated with high accuracy using bisection search, since $f$ is monotonic. Together with Thompson sampling, this makes up BQL (\cref{alg:bql}).

\begin{algorithm}
  \caption{Bayesian Q-Learning (BQL)}\label{alg:bql}
  \begin{algorithmic}[1]
\State Initialise posterior parameters $\thetamczo = (\mu_{0_{\s, \ac}}, \lambda_{\s, \ac}, \alpha_{\s, \ac}, \beta_{\s, \ac})$ for each $(\s, \ac)$
 \State Observe initial state $\s_1$
 \For{time-step $\in \{0, 1, ..., T_{\max} - 1\}$ }
  \State Thompson-sample $\ac_t$ using $p(\thetamczo | \data)$ and observe next state $\s_{t+1}$ and reward $r_t$
  \State $\thetamczo \leftarrow $ Updated params. using Monte Carlo on \cref{eq:bqlupdate}
 \EndFor
  \end{algorithmic}
\end{algorithm}

As more data is observed and the posteriors become narrower, we hope that the agent will converge to greedy behaviour and find the optimal policy.

\subsection{Posterior Sampling for Reinforcement Learning} \label{app:psrl}

For PSRL in the tabular setting we follow the approach of \cite{psrl}, and use a Categorical-Dirichlet model for $\mct$ and a Gaussian-NG model for $\mcr$. The posterior is updated after each episode or user-defined number of time-steps, such as the number of states in the MDP. Once the dynamics and rewards have been sampled:
$$\thetamcthat \sim \thetamctpost,~~ \thetamcrhat \sim \thetamcrpost,$$
we can solve for $\hat{Q}^* | \thetamcthat, \thetamcrhat $ and $\hat{\pi}^* | \thetamcthat, \thetamcrhat$ by dynamical programming in the episodic setting or by Policy Iteration (PI) in the continuing setting. \Cref{alg:psrl} gives a pseudocode listing.

\begin{algorithm}
  \caption{Posterior Sampling Reinforcement Learning (PSRL)}\label{alg:psrl}
  \begin{algorithmic}[1]
\State Input data $\data$ and posteriors $p(\thetamct | \data)$, $p(\thetamcr | \data)$
 \For{t $ \in \{0, 1, ..., T_{max} - 1\}$}
 	\If{ $t \texttt{ \% } T_{\text{update}} \texttt{ == } 0$}
	\State Update $p(\thetamct | \data)$ and $p(\thetamcr | \data)$ using observed data
	\State Sample $\thetamcthat \sim p(\thetamct | \data)$ and $\thetamcrhat \sim p(\thetamcr | \data)$
	\State Solve Bellman equation for $\hat{Q}^*_{\s, \ac}$ by PI and $\hat{\pi}^*_{\s} \leftarrow \argmax_{\ac} \hat{Q}^*_{\s, \ac}$
	\EndIf
	\State Observe state $\s_t$ and take action $\hat{\pi}^*_{\s_t}$
	\State Store $(\s_t, \ac_t, r_t, \s_{t+1})$ in $\data$
 \EndFor
\end{algorithmic}
\end{algorithm}

As with BQL, the posteriors will become narrower as more data are observed and the agent will converge to the true optimal policy. \cite{psrl} formalise this intuition and prove that the regret of PSRL grows sub-linearly with the number of time-steps.

\subsection{The uncertainty Bellman equation}\label{app:ube}
To derive the UBE, \cite{ube} make the following assumptions:

\textbf{Assumption 1:} The MDP is a directed acyclic graph (DAG), so each state-action can be visited at most once per episode. Any finite MDP can be turned into a DAG by a process called \textit{unrolling}: creating $T$ copies of each state for each time $t = 1, ..., T$. \cite{ube} thus consider:
\begin{align} \label{app:eq:bet}
\mu_{z^\pi_{\s, \ac, t}} = \E_{r, \ns}\sqb{\rsanbt + \gamma \max_{\ac'}  \mu_{z^\pi_{\ns, \ac', t + 1}} \big| \pi, \thetamct, \thetamcr}, \text{ where } \mu_{z^\pi_{\s, \ac, T + 1}} = 0, \forall (\s, \ac)
\end{align}
Unrolling increases data sparsity since roughly $T$ more data would must be observed to narrow down individual posteriors by the same amount as when no unrolling is used. Further, this approach would confine the UBE to episodic tasks, so the authors choose to violate this assumption in their experiments and we follow the same approach.

\textbf{Assumption 2:} The mean immediate rewards of the MDP are bounded within $[-R_{max}, R_{max}]$, so the $\mu_{z^\pi_{\s, \ac, t}}$ values can be upper-bounded by $T R_{\max}$ in the episodic setting and by $R_{\max} / (1 - \gamma)$ in the continuing setting. We write this upper bound as $Q_{max}$.


Taking variances across the BE, the authors derive the upper bound:
\begin{align} \label{eq:bellunc}
\underbrace{\Var_{\thetamct, \thetamcr} \big[ \mu_{z^\pi_{\s, \ac, t}} \big]}_{\text{Epistemic unc. in $\mu_{z^\pi_{\s, \ac, t}}$}} \leq \nu^{\pi}_{\s, \ac, t} + \E_{\s', \ac'} \Big[ \underbrace{\E_{\thetamct} \big[p(\s' | \s, \ac, \thetamct) \big] }_{\text{Posterior predictive dynamics}}~\underbrace{\Var_{\thetamct, \thetamcr} \big[ \mu_{z^\pi_{\ns, \ac', t + 1}} \big]}_{\text{Epistemic unc. in $\mu_{z^\pi_{\ns, \ac', t + 1}}$}} \big|~\pi \Big]
\end{align}
\begin{align} \label{eq:localunc}
\text{where}~~ \nu^{\pi}_{\s, \ac, t} &=  \underbrace{\Var_{\thetamcr} \sqb{\mu_{r_{\s, \ac, \ns, t}}}}_{\text{Epistemic unc. in $\mu_{r_{\s, \ac, \ns, t}}$}} +~Q_{max}^2 \sum_{\s'} \frac{\Var_{\thetamct} \big[p(\s' | \s, \ac, \thetamct) \big]}{\E_{\thetamct} \big[p(\s' | \s, \ac, \thetamct) \big]}
\end{align}
The bounding term in ineq. \ref{eq:bellunc} is the sum of a $\nu^{\pi}_{\s, \ac, t}$ term plus an expectation term. The former depends on quantities local to $(\s, \ac)$, and is called the \textit{local uncertainty}. The latter term in \cref{eq:bellunc} is an expectation of the next-step epistemic uncertainty weighted by the posterior predictive dynamics. It propagates the epistemic uncertainty across state-actions. Defining $\mathcal{U}^\pi_{t}$ as:
\begin{align*}
\Ubell u_{\s, \ac, t}^\pi  = \nu^{\pi}_{\s, \ac, t} + \E_{\s', \ac'} \big[ \E_{\thetamct} \big[p(\s' | \s, \ac, \thetamct)\big] u^\pi_{\ns, \ac', t + 1} | \pi \big],
\end{align*}
the authors arrive at the UBE:
\begin{gather*}
u_{\s, \ac, t}^\pi = \mathcal{U}^\pi_{t} u_{\s, \ac, t + 1}^\pi,\text{ where } u_{\s, \ac, T + 1}^\pi = 0
\end{gather*}
If unrolling is not applied, the bound $u_{\s, \ac, t}^\pi$ is no longer strictly true and the UBE becomes a heuristic:
\begin{align}\label{app:eq:ubenounroll}
u_{\s, \ac}^\pi = \mathcal{U}^\pi u_{\s, \ac}^\pi.
\end{align}
We can first obtain the greedy policy ${\pi^*}$, through PI. Subsequently we solve for the fixed point of the UBE, without unrolling, to obtain $u_{\s, \ac}^*$. Introducing the scaling factor $\zeta$ we finally use $u_{\s, \ac}^*$ for Thompson sampling from a diagonal gaussian. This amounts to a factored posterior approximation. \Cref{ube:algo} shows the complete process.

\begin{algorithm}
  \caption{Uncertainty Bellman Equation with Thompson sampling}
  \begin{algorithmic}[1]\label{ube:algo}
\State Input data $\data$ and posteriors $p(\thetamct | \data)$, $p(\thetamcr | \data)$
 \For{$t \in \{0, 1, ..., T_{\max} - 1\}$ }
 	\If{ $t \texttt{ \% } T_{\text{update}} \texttt{ == } 0$}
	\State Update $p(\thetamct | \data)$ and $p(\thetamcr | \data)$ using observed data
	\State Solve for greedy policy ${\pi^*}$ by PI
	\State Solve for $u_{\s, \ac}^*$ in \cref{app:eq:ubenounroll}
	\EndIf
 	\State Observe $\s_t$
	\State Thompson-sample $\ac_t = \argmax_{\ac} \big(\mu_{z^*_{\s, \ac}} + \zeta \epsilon_{\s, \ac}~(u_{\s, \ac}^*)^{1/2}\big)$, $\epsilon_{\s, \ac} \sim \mcn(0, 1)$
	\State Observe $\s_{t+1}, r_{t}$ and store $(\s_{t}, \ac_t, r_{t}, \s_{t+1})$ in $\data$
\EndFor
\end{algorithmic}
\end{algorithm}

\noindent Note that as the posterior variance collapses to 0 in the limit of infinite data, $\nu^{\pi}_{\s, \ac, t} \to 0$ because both terms in \cref{eq:localunc} also tend to 0. Therefore, we also have $u_{\s, \ac, t}^\pi \to 0$, and the agent will automatically transition to greedy behaviour.

\subsection{Moment matching across the BE}\label{app:mm}

Starting from the Bellman relation for $\zb$:
\begin{align*}
\zb = \rsanb + \gamma \znb,
\end{align*}
where $\bm{s}' \sim p(\ns | \s, \ac)$, $\ac' \sim \pi(\ns)$, we require equality between the first and second order moments\footnote{Expectations and variances are over the posteriors of the subscript variables conditioned on data $\data$.}:
\begin{align}
\E_{z, \thetamcw} \sqb{\zb} &= \E_{r, \thetamcr, z, \thetamcz, \ns, \thetamct, \ac'} \sqb{\rsanb + \gamma \znb | \pi} \label{eq:z1} \\
\Var_{z, \thetamcw} \sqb{\zb} &= \Var_{r, \thetamcr, z, \thetamcz, \ns, \thetamct, \ac'} \sqb{\rsanb + \gamma \znb | \pi} \label{eq:z2}
\end{align}
\Cref{eq:z1} is the familiar BE for $\Q$, which can be used to compute the greedy policy by PI. \Cref{eq:z2} can be expanded on both sides to express a similar equality between variances. First, using the law of total variance on the LHS:
\begin{align*}
\underbrace{\Var_{z, \thetamcz} \sqb{\zb}}_{\text{Total value variance}} &= \underbrace{\Var_{\thetamcz} \sqb{\E_z\sqb{\zb | \thetamcz}}}_{\text{Epistemic value variance}} + \underbrace{\E_{\thetamcz} \sqb{\Var_z\sqb{\zb | \thetamcz}}}_{\text{Aleatoric value variance}}.
\end{align*}
Second, we expand the RHS of \cref{eq:z2} and obtain
\begin{align}
\underbrace{\Var_{z, \thetamcw} \sqb{\zb}}_{\text{Total value variance}} = &\underbrace{\Var_{r, \thetamcr, \ns, \thetamct}[\rsanb] }_{\text{Reward variance}} + 2\gamma \underbrace{\Cov_{r, \thetamcr, z, \thetamcz, \ns, \thetamct, \ac'}[\rsanb, \znb]}_{\text{Reward-value covariance}} \nonumber \\
+ & \gamma^2 \underbrace{\Var_{z, \thetamcz, \ns, \thetamct, \ac'}[\znb]}_{\text{Next-step value variance}}. \label{eq:z2expanded}
\end{align}
Each of the terms in \cref{eq:z2expanded} contains contributions from aleatoric as well as epistemic sources, which can be separated using the laws of total variance and total covariance (\cite{weiss})- the decompositions are straightforward but lengthy and are included in the supporting material.

Since each uncertainty comes from a different source, we argue that one BE should be satisfied for each. We therefore obtain the following consistency equation for the epistemic terms:
\begin{align}\label{eq:epistemicmm}
\underbrace{\Var_{\thetamcz} \sqb{\E_z\sqb{\zb | \thetamcz}}}_{\text{Epistemic action-return unc.}} =& \underbrace{\Var_{\thetamct} \sqb{ \E_{\ns, r, \thetamcr} \sqb{\rsanb \big| \thetamct}}}_{\substack{\text{Epistemic reward unc. from}  \\ \text{dynamics unc.}}} \\
& + \underbrace{\E_{\ns, \thetamct}\sqb{ \Var_{\thetamcr} \sqb{ \E_r \sqb{\rsanb \big| \ns, \thetamct, \thetamcr}}}}_{\substack{\text{Epistemic rewards unc. from} \\ \text{rewards unc.}}} + \nonumber \\
& + 2 \gamma \underbrace{\Cov_{\thetamct}\sqb{\E_{\ns, r, \thetamcr}\sqb{\rsanb | \thetamct}, \E_{\ns, z, \thetamcz, \ac'}\sqb{\znb | \thetamct}}}_{\substack{\text{Epistemic reward and action-return covariance} \\ \text{from dynamics unc.}}} \nonumber \\
& + \gamma^2 \underbrace{\Var_{\thetamct} \sqb{\E_{\ns, z, \thetamcz, \ac'} \sqb{\znb | \thetamct}}}_{\substack{\text{Epistemic action-return unc. from} \\ \text{dynamics unc.}}} \nonumber \\
& +  \gamma^2 \underbrace{\E_{\ns, \thetamct, \ac'}\sqb{\Var_{\thetamcz} \sqb{ \E_z \sqb{\znb | \ns, \thetamcz}}}}_{\substack{\text{Epistemic action-return unc. from} \\ \text{state-return unc.}}} \nonumber
\end{align}
With the exception of the last term in \cref{eq:epistemicmm}, all RHS terms can be readily computed provided we already have $\E_{\ns, z, \thetamcz}\sqb{\znb | \thetamct}$ from \cref{eq:z1}. We observe that the last term is the same as the LHS term, except it has been smoothed out w.r.t. the next-state posterior predictive. Therefore, \cref{eq:epistemicmm} is a system of linear equations which can be solved in $O(|\mcs|^3|\mca|^3)$ time for the epistemic uncertainty in $\mu_{\zb}$. The latter can be subsequently used for Thompson sampling from a diagonal Gaussian:
\begin{gather*}
\ac = \argmax_{\ac'} \big(\mu_{z^{{*}}_{\s, \ac'}} + \zeta \epsilon_{\s, \ac'}~\tilde{\sigma}_{z^*_{\s, \ac'}} \big),\\
\text{ where } \epsilon_{\s, \ac} \sim \mc{N}(0, 1), \text{ and }~\tilde{\sigma}^2_{z^*_{\s, \ac}} = \Var_{\thetamcz} \sqb{\E_z\sqb{\zb | \thetamcz}},
\end{gather*}
where $\pi = \pi^*$ has been used. $\zeta$ can be adjusted as with the UBE, although we do not find this is necessary in our tabular experiments and use $\zeta = 1.0$ throughout.

\begin{algorithm}
  \caption{Moment Matching with Thompson sampling}\label{alg:tmm}
  \begin{algorithmic}[1]
\State Input data $\data$ and posteriors $p(\thetamct | \data)$, $p(\thetamcr | \data)$
 \For{$t \in \{0, 1, ..., T_{\max} - 1\}$ }
 	\If{ $t \texttt{ \% } T_{\text{update}} \texttt{ == } 0$}
	\State Update $p(\thetamct | \data)$ and $p(\thetamcr | \data)$ using observed data
	\State Solve for greedy policy ${\pi^*}$ by PI
	\State Compute epistemic uncertainty $\tilde{\sigma}_{z^*_{\s, \ac}}^2$ by solving \cref{eq:epistemicmm}
	\EndIf
 	\State Observe $\s_t$
	\State Thompson-sample and execute $\ac_t = \argmax_{\ac} \big(\mu_{z_{\s_t, \ac}^*} + \epsilon_{\s_t, \ac}~\tilde{\sigma}_{z^*_{\s_t, \ac}}\big)$
	\State Observe $\s_{t+1}, r_{t}$ and store $(\s_{t}, \ac_t, r_{t}, \s_{t+1})$ in $\data$
 \EndFor
\end{algorithmic}
\end{algorithm}

\clearpage

\section{Additional environment details} \label{app:env}

\subsection{DeepSea}

Our DeepSea MDP (\cref{deepseaMDP}) is a variant of the ones used in \cite{rand_val_func, deepsea}. The agent starts from $\s_1$ and can choose swim-\textit{left} or swim-\textit{right} from each of the $N$ states in the environment.

Swim-\textit{left} always succeeds and moves the agent to the left, giving $r = 0$ (red transitions). Swim-\textit{right} from $\s_1, ..., \s_{N-1}$ succeeds with probability $1 - 1/N$, moving the agent to the right and otherwise fails moving the agent to the left (blue arrows), giving $r = - \delta$ regardless of whether it succeeds. A successful swim-\textit{right} from $\s_N$ moves the agent back to $\s_1$ and gives $r = 1$. We choose $\delta$ so that \textit{right} is always optimal\footnote{We choose $\delta = 0.1 \times \exp^{-N / 4}$ in our experiments, which guarantees \textit{right} is optimal at least up to $N = 40$.}.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[scale=0.8]
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (a) at (0,0) {$\s_1$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (b) at (2.5,0) {$\s_2$};
\node (c) at (5,0) {};
\node (d) at (7,0) {};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (e) at (9.5,0) {$\s_{N-1}$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (f) at (12,0) {$\s_N$};

\draw[bend left,->,thick, blue]  ($(a) + (0.7, 0.1)$) to node [auto] {} ($(b) - (0.7, -0.1)$);
\draw[bend left,->,thick, blue]  ($(b) - (0.7, 0.1)$) to node [auto] {} ($(a) + (0.7, -0.1)$);
\draw[bend left,->,thick, red]  ($(b) - (0.2, 0.7)$) to node [auto] {} ($(a) + (0.2, -0.7)$);
\node[anchor=south] at ($(a) + (1.25, 0.2)$) {\scriptsize{$1 - 1/N$}};
\node[anchor=north] at ($(a) + (1.25, -0.2)$) {\scriptsize{$1/N$}};
\node[anchor=north] at ($(a) + (1.25, -1.0)$) {\scriptsize{$1$}};

\draw[bend left,->,thick, blue]  ($(b) + (0.7, 0.1)$) to node [auto] {} ($(c) - (0.7, -0.1)$);
\draw[bend left,->,thick, blue]  ($(c) - (0.7, 0.1)$) to node [auto] {} ($(b) + (0.7, -0.1)$);
\draw[bend left,->,thick, red]  ($(c) - (0.2, 0.7)$) to node [auto] {} ($(b) + (0.2, -0.7)$);
\node[anchor=south] at ($(b) + (1.25, 0.2)$) {\scriptsize{$1 - 1/N$}};
\node[anchor=north] at ($(b) + (1.25, -0.2)$) {\scriptsize{$1/N$}};
\node[anchor=north] at ($(b) + (1.25, -1.0)$) {\scriptsize{$1$}};

\draw[bend left,->,thick, blue]  ($(d) + (0.7, 0.1)$) to node [auto] {} ($(e) - (0.7, -0.1)$);
\draw[bend left,->,thick, blue]  ($(e) - (0.7, 0.1)$) to node [auto] {} ($(d) + (0.7, -0.1)$);
\draw[bend left,->,thick, red]  ($(e) - (0.2, 0.7)$) to node [auto] {} ($(d) + (0.2, -0.7)$);
\node[anchor=south] at ($(d) + (1.25, 0.2)$) {\scriptsize{$1 - 1/N$}};
\node[anchor=north] at ($(d) + (1.25, -0.2)$) {\scriptsize{$1/N$}};
\node[anchor=north] at ($(d) + (1.25, -1.0)$) {\scriptsize{$1$}};

\draw[bend left,->,thick, blue]  ($(e) + (0.7, 0.1)$) to node [auto] {} ($(f) - (0.7, -0.1)$);
\draw[bend left,->,thick, blue]  ($(f) - (0.7, 0.1)$) to node [auto] {} ($(e) + (0.7, -0.1)$);
\draw[bend left,->,thick, red]  ($(f) - (0.2, 0.7)$) to node [auto] {} ($(e) + (0.2, -0.7)$);
\node[anchor=south] at ($(e) + (1.25, 0.2)$) {\scriptsize{$1 - 1/N$}};
\node[anchor=north] at ($(e) + (1.25, -0.2)$) {\scriptsize{$1/N$}};
\node[anchor=north] at ($(e) + (1.25, -1.0)$) {\scriptsize{$1$}};

\draw [->, thick, red] (a) to[out=-160, in=-90,looseness=1.5] ($(a) - (1.5, 0)$)    to[out=90,in=160,looseness=1.5] (a);
\draw [->, thick, blue] (a) to[out=60, in=0,looseness=1.5] ($(a) + (0, 1)$)    to[out=180,in=120,looseness=1.5] (a);
\draw [->, thick, blue] (f) to[out=-45, in=0,looseness=1] (6, -2.5)    to[out=180,in=-135,looseness=1] (a);
\node[anchor=south] at ($(a) + (-1.10, 0.40)$) {\scriptsize{$1$}};
\node[anchor=south] at ($(a) + (0.00, 1.00)$) {\scriptsize{$1/N$}};
\node[anchor=south] at (6, 1.3) {\scriptsize{$r = 0$} \text{for all swim-\textit{left} actions (red arrows)}};
\node[anchor=south] at (6, 0.9) {\scriptsize{$r = -\delta$} \text{for all swim-\textit{right} actions (blue arrows), except bottom}};
\node[anchor=south] at (6, -2.5) {\begin{tabular}{c} \scriptsize{$r = 1$} \\  \scriptsize{$1 - 1/N$} \end{tabular}};

\draw[bend left,->,thick, blue]  ($(f) - (0.7, 0.1)$) to node [auto] {} ($(e) + (0.7, -0.1)$);

\node (e) at (6,0) {$\hdots$};
\end{tikzpicture}
\caption[DeepSea MDP]{DeepSea MDP from the continuing setting, modified from \cite{deepsea}. Blue arrows correspond to swim-\textit{right} (optimal) and red arrows to swim-\textit{left} (sub-optimal).}\label{deepseaMDP}
\end{figure}
This environment is designed to test whether the agent continues exploring despite receiving negative rewards. Sustained exploration becomes increasingly important for large $N$. As argued in \cite{iothesis}, in order to avoid exponentially poor performance, exploration in such chain-like environments must be guided by uncertainty rather than randomness.

\subsection{WideNarrow}

The WideNarrow MDP (\cref{widenarrowMDP}) has $2N + 1$ states and deterministic transitions. Odd states except $\s_{2N + 1}$ have $W$ actions, out of which one gives $r \sim \mc{N}(\mu_h, \sigma^2)$ whereas all others give $r \sim \mc{N}(\mu_l, \sigma^2)$, with $\mu_l < \mu_h$. Even states have a single action also giving $r \sim \mc{N}(\mu_l, \sigma^2)$. In our experiments we use $\mu_h = 0.5, \mu_l = 0$ and $\sigma_h = \sigma_l = 1$.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[scale=0.6]
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (a) at (0,0) {$\s_1$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (b) at (2.5,0) {$\s_2$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (c) at (5,0) {$\s_3$};

\node (z) at (1.25,0.1) {$\vdots$};

\draw [->, thick, blue] (a) to[out=70, in=-180,looseness=0.75] (1.25, 3) to[out=0,in=110,looseness=0.75] (b);
\draw [->, thick, red] (a) to[out=50, in=-180,looseness=0.9] (1.25, 1.6) to[out=0,in=130,looseness=0.9] (b);
\draw [->, thick, red] (a) to[out=-50, in=-180,looseness=0.9] (1.25, -1.6) to[out=0,in=-130,looseness=0.9] (b);
\draw [->, thick, red] (a) to[out=-70, in=-180,looseness=0.75] (1.25, -3) to[out=0,in=-110,looseness=0.75] (b);

\draw [->, thick, red] (b) to (c);

\draw [->, thick, blue] (c) to[out=70, in=-180,looseness=0.75] ($(c) + (1, 2)$);
\draw [->, thick, red] (c) to[out=50, in=-180,looseness=0.75] ($(c) + (1, 1)$);
\draw [->, thick, red] (c) to[out=-50, in=-180,looseness=0.75] ($(c) + (1, -1)$);
\draw [->, thick, red] (c) to[out=-70, in=-180,looseness=0.75] ($(c) + (1, -2)$);

\node (g) at (7.00, 0.0) {$\hdots$};

\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (d) at (8.5,0) {$\s_{2N - 1}$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (e) at (11,0) {$\s_{2N}$};
\node[draw,thick,circle,minimum size=1.1cm,inner sep=0pt, fill=white] (f) at (13.5,0) {$\s_{2N + 1}$};

\draw [->, thick, blue] (d) to[out=70, in=-180,looseness=0.75] ($(d) + (1.25, 3)$) to[out=0,in=110,looseness=0.75] (e);
\draw [->, thick, red] (d) to[out=50, in=-180,looseness=0.9] ($(d) + (1.25, 1.6)$) to[out=0,in=130,looseness=0.9] (e);
\draw [->, thick, red] (d) to[out=-50, in=-180,looseness=0.9] ($(d) + (1.25, -1.6)$) to[out=0,in=-130,looseness=0.9] (e);
\draw [->, thick, red] (d) to[out=-70, in=-180,looseness=0.75] ($(d) + (1.25, -3)$) to[out=0,in=-110,looseness=0.75] (e);

\draw [->, thick, red] (e) to (f);
\node (g) at (9.75, 0.1) {$\vdots$};

\draw [->, thick, red] (f) to[out=-90, in=0,looseness=1.3] (6.75, -4) to[out=180,in=-90,looseness=1.3] (a);

\node[anchor=south] at (7, 4.6) {\scriptsize{$r \sim \mc{N}(\mu_h, \sigma^2_h)$} \text{for blue arrow transitions}};
\node[anchor=south] at (7, 3.9) {\scriptsize{$r \sim \mc{N}(\mu_l, \sigma^2_l)$} \text{for red arrow transitions}};
\end{tikzpicture}
\caption[WideNarrow MDP]{The WideNarrow MDP. All transitions are deterministic.}\label{widenarrowMDP}
\end{figure}

In general, the returns from different state-actions will be correlated under the posterior. Here, consider $(\s_1, \ac_1)$ and $(\s_1, \ac_2)$:
\begin{align}
\Cov_{z, \btheta} \sqb{ z^*_{\s_1, \ac_1}, z^*_{\s_1, \ac_2}} = &\Cov_{r, z, \btheta} \sqb{ r_{\s_1, \ac_1, \ns} + \gamma z^*_{\ns, \ac'}, ~r_{\s_1, \ac_2, \s{''}} + \gamma z^*_{\s{''}, \ac{''}}} \label{eq:cov_demo} \\
 =& \cancel{\Cov_{r, z, \btheta} \Big[ r_{\s_1, \ac_1, \ns}, r_{\s_1, \ac_2, \s{''}}\Big]} + \gamma \Cov_{r, \btheta} \sqb{r_{\s_1, \ac_1, \ns} , z^*_{\s{''}, \ac{''}} }\nonumber \\
&+ \gamma \Cov_{r, z, \btheta} \sqb{ r_{\s_1, \ac_2, \s{''}} , z^*_{\s{''}, \ac{''}} } + \gamma^2 \Cov_{z, \btheta} \sqb{z^*_{\ns, \ac'},  z^*_{\s{''}, \ac{''}}}\nonumber
\end{align}
where $\btheta$ loosely denotes all modelling parameters, $\s'$ denotes the next-state from $\s_1, \ac_1$, $\s{''}$ denotes the next-state from $\s_1, \ac_2$ and $\ac{'}, \ac{''}$ denote the corresponding next-actions. Although the remaining three terms are non-zero under the posterior, BQL, UBE and MM ignore them, instead sampling from a factored posterior. The WideNarrow environment enforces strong correlations between these state actions, through the last term in \cref{eq:cov_demo}, allowing us to test the impact of a factored approximation.

\subsection{PriorMDP}

The aforementioned MDPs have very specific and handcrafted dynamics and rewards, so it is interesting to also compare the algorithms on environments which lack this sort of structure. For this we sample finite MDPs with $N_s$ states and $N_a$ action from a prior distribution, as in \cite{psrl}. $\mct$ is a Categorical with parameters $\{\bs{\eta_{\s, \ac}}\}$ with:
\begin{align*}
\bs{\eta}_{\s, \ac} \sim \text{Dirichlet}(\bs{\kappa}_{\s, \ac}),
\end{align*}
with pseudo-count parameters $\bs{\kappa}_{\s, \ac} = \bm{1}$, while $\mcr \sim \mc{N}(\mu_{\s, \ac}, \tau_{\s, \ac}^{-1})$ with:
\begin{align*}
\mu_{\s, \ac}, \tau_{\s, \ac} \sim NG(\mu_{\s, \ac}, \tau_{\s, \ac} | \mu, \lambda, \alpha, \beta) \text{ with } (\mu, \lambda, \alpha, \beta) = (0.00, 1.00, 4.00, 4.00).
\end{align*}
We chose these hyperparameters because they give $Q^*$-values in a reasonable range.

\clearpage
\section{Supplementary figures}

\subsection{Regret summaries}

The following plots summarise the regret of each algorithm to the oracle agent. The regrets have been normalised by the total reward received by the agent, to make the numbers comparable across environments.

\begin{figure}[h!]
\centering
\begin{subfigure}{0.8\textwidth}
\includegraphics[width=\linewidth]{img/regret_summary_deepsea.pdf}
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{Summary of regret performances to oracle on DeepSea.}\label{deepsea_regret_summary}
\end{figure}

\clearpage

\subsection{DeepSea}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_5-qestimates-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_5-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{QL $Q$-estimates and regret on DeepSea.}\label{ql_deepsea_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-posterior-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{BQL posterior and regret on DeepSea.}\label{bql_deepsea_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-posterior-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{PSRL posterior and regret on DeepSea.}\label{psrl_deepsea_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-posterior-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on DeepSea.}\label{ube_deepsea_visual1}
\end{figure}

\begin{figure}[h!]
\centering
\vspace{-1cm}
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-posterior-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on DeepSea.}\label{ube_deepsea_visual01}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.7\textwidth}
\includegraphics[width=\linewidth]{img/ube-uncertainty-terms.pdf}
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{Contributions to the local variance $\nu^*_{\s, \ac}$ by the reward and the $Q_{max}$ term. This plot corresponds to \cref{ube_deepsea_visual01}. Note the logarithmic scale.}\label{ube_unc_terms}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-posterior-deepsea-4.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-regret-deepsea-4.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{MM posterior and regret on DeepSea.}\label{mm_deepsea_visual}
\end{figure}

% ============================================================
% WideNarrow
% ============================================================
\clearpage

\subsection{WideNarrow}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_2-qestimates-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_2-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{QL $Q$-estimates and regret on WideNarrow.}\label{ql_widenarrow_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/bql-1_5-4_0-3_0-3_0-posterior-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/bql-1_5-4_0-3_0-3_0-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{BQL posterior and regret on WideNarrow.}\label{bql_widenarrow_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-posterior-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{PSRL posterior and regret on WideNarrow.}\label{psrl_widenarrow_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-posterior-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on WideNarrow.}\label{ube_widenarrow_visual1}
\end{figure}

\begin{figure}[h!]
\centering
\vspace{-1cm}
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-posterior-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on WideNarrow.}\label{ube_widenarrow_visual01}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-posterior-widenarrow-1-6.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-regret-widenarrow-1-6.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{MM posterior and regret on WideNarrow.}\label{mm_widenarrow_visual}
\end{figure}

\clearpage

\begin{figure}[h!]
\centering
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-scatter-widenarrow-1-6-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-scatter-widenarrow-1-6-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-scatter-widenarrow-1-6-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-scatter-widenarrow-1-6-1000.pdf}
\end{subfigure}\\
\captionsetup{width=0.9\linewidth}
\caption{Correlation plots for WideNarrow at time step $t = 1,000$.}\label{correlations_widenarrow_500}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-scatter-widenarrow-1-6-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-scatter-widenarrow-1-6-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-scatter-widenarrow-1-6-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-scatter-widenarrow-1-6-5000.pdf}
\end{subfigure}\\
\captionsetup{width=0.9\linewidth}
\caption{Correlation plots for WideNarrow at time step $t = 5,000$.}\label{correlations_widenarrow_2500}
\end{figure}

% ============================================================
% PriorMDP
% ============================================================

\clearpage

\subsection{PriorMDP}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_2-qestimates-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ql-0_2-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{QL $Q$-estimates and regret on PriorMDP.}\label{ql_priormdp_visual}
\end{figure}


\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-posterior-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{BQL posterior and regret on PriorMDP.}\label{bql_priormdp_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-posterior-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{PSRL posterior and regret on PriorMDP.}\label{psrl_priormdp_visual}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-posterior-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-1_0-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on PriorMDP.}\label{ube_priormdp_visual1}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-posterior-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{UBE posterior and regret on PriorMDP.}\label{ube_priormdp_visual1}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{0.65\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-posterior-priormdp-4-2.pdf}
\end{subfigure}
\begin{subfigure}{0.34\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-regret-priormdp-4-2.pdf}~\\~\\
\end{subfigure}
\captionsetup{width=0.9\linewidth}
\caption{MM posterior and regret on PriorMDP.}\label{mm_priormdp_visual}
\end{figure}


\clearpage

\begin{figure}[h!]
\centering
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-scatter-priormdp-4-2-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-scatter-priormdp-4-2-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-scatter-priormdp-4-2-1000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-scatter-priormdp-4-2-1000.pdf}
\end{subfigure}\\
\captionsetup{width=0.9\linewidth}
\caption{Correlation plots for PriorMDP at time step $t = 1,000$.}\label{correlations_priormdp_500}
\end{figure}

\begin{figure}[h!]
\centering
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/bql-0_0-4_0-3_0-3_0-scatter-priormdp-4-2-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/psrl-0_0-4_0-3_0-3_0-scatter-priormdp-4-2-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/ube-0_0-4_0-3_0-3_0-0_1-scatter-priormdp-4-2-5000.pdf}
\end{subfigure}\\
~\\
~\\
\begin{subfigure}{1.0\textwidth}
\includegraphics[width=\linewidth]{img/mm-0_0-4_0-3_0-3_0-1_0-scatter-priormdp-4-2-5000.pdf}
\end{subfigure}\\
\captionsetup{width=0.9\linewidth}
\caption{Correlation plots for PriorMDP at time step $t = 5,000$.}\label{correlations_priormdp_2500}
\end{figure}

\end{appendices}

\end{document}
